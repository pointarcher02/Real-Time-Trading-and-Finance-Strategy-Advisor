{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "# from untrade.client import Client\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_PATH = \"BTC_2019_2023_1h.csv\"  # Path to your dataset\n",
    "MODEL_PATH = \"TFT_FINAL_MODEL_PATH_.pth\"  # Path to save your trained model\n",
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "batch_size = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load data\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(data):\n",
    "    # Basic preprocessing steps (example)\n",
    "    # add time index\n",
    "    data[\"time_idx\"] = data.index\n",
    "    data['datetime']=pd.to_datetime(data['datetime'])\n",
    "    # data.rename(columns={\"Unnamed: 0\": \"time_idx\"}, inplace=True)\n",
    "    # add additional features\n",
    "    data['date']=pd.to_datetime(data['datetime'])\n",
    "    # data['month'] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "    data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "    data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "    data['Target'] = data['close'].shift(-1) - data['close']\n",
    "    data['TargetClass'] = data['Target'].apply(lambda x: 1 if x > 0 else 0).astype(float)\n",
    "    data['group_id']='BTC'\n",
    "    data['predictions']=-1\n",
    "    data['predictions_y']=-1\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to train the model\n",
    "def train_model(trainer,tft,train_dataloader,val_dataloader):\n",
    "    # Train your model here\n",
    "        \n",
    "        trainer.fit(\n",
    "                   tft,\n",
    "                   train_dataloaders=train_dataloader,\n",
    "                   val_dataloaders=val_dataloader,\n",
    "                    )\n",
    "        \n",
    "        \n",
    "        return tft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to save the model\n",
    "def save_model(model, path):\n",
    "    # Save model and scaler\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_signals(data,n,pred):\n",
    "    \"\"\"\n",
    "    Generates trading signals based on percentage change.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): Array of numerical values.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of trading signals (2, 1, 0, -1, -2).\n",
    "    \"\"\"\n",
    "\n",
    "    signals = np.zeros_like(data, dtype=int)\n",
    "    current_position = 0  # 0: neutral, 1: long, -1: short\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        percentage_change = ((pred[i] - data[i - 1]) / pred[i])*100\n",
    "\n",
    "        if percentage_change > n:\n",
    "            if current_position == 0:  # Neutral\n",
    "                signals[i] = 1\n",
    "                current_position = 1 # Update position to long\n",
    "            elif current_position == -1:  # Currently short\n",
    "                signals[i] = 2\n",
    "                current_position = 1  # Update position to neutral\n",
    "            elif current_position == 1:  # Currently short\n",
    "                signals[i] =  0\n",
    "\n",
    "        elif percentage_change < -1*n:\n",
    "            if current_position == 0:  # Neutral\n",
    "                signals[i] = -1\n",
    "                current_position = -1\n",
    "            elif current_position == 1:  # Currently long\n",
    "                signals[i] = -2\n",
    "                current_position = -1  # Update position to short\n",
    "            elif current_position == -1:  # Currently short\n",
    "                signals[i] =  0\n",
    "\n",
    "        elif (percentage_change < n and percentage_change>0):\n",
    "            if current_position == 0:  # Neutral\n",
    "                signals[i] = 0\n",
    "                current_position = 0\n",
    "            elif current_position == 1:  # Currently long\n",
    "                signals[i] = -1\n",
    "                current_position = 0\n",
    "            elif current_position == -1:  # Currently long\n",
    "                signals[i] = 1\n",
    "                current_position = 0\n",
    "\n",
    "        elif (percentage_change > -1*n and percentage_change<0):\n",
    "            if current_position == 0:\n",
    "                signals[i] = 0\n",
    "                # current_position = -1\n",
    "            elif current_position == 1:\n",
    "                signals[i] = -1\n",
    "                current_position = 0\n",
    "            elif current_position == -1:\n",
    "                signals[i] = 1\n",
    "                current_position = 0\n",
    "\n",
    "    return signals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to generate backtest result\n",
    "def backtest(signals):\n",
    "    \"\"\"\n",
    "    Perform backtesting using the untrade SDK.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file_path (str): Path to the CSV file containing historical price data and signals.\n",
    "\n",
    "    Returns:\n",
    "    - result (generator): Generator object that yields backtest results.\n",
    "    \"\"\"\n",
    "    # Create an instance of the untrade client\n",
    "    client = Client()\n",
    "\n",
    "    # Perform backtest using the provided CSV file path\n",
    "    result = client.backtest(\n",
    "        jupyter_id=\"jenish\",  # the one you use to login to jupyter.untrade.io\n",
    "        file_path = signals,\n",
    "        leverage=1,  # Adjust leverage as needed\n",
    "        result_type=\"Q\", # Optional\n",
    "    )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    data = load_data(DATA_PATH)\n",
    "\n",
    "    # Preprocess data\n",
    "    data = preprocess_data(data)\n",
    "\n",
    "    training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "    training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"close\",\n",
    "    group_ids=[ \"group_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"group_id\"],\n",
    "    static_reals=[],\n",
    "    # time_varying_known_categoricals=[\"time_idx\", \"month\"],\n",
    "    time_varying_known_reals=[\"time_idx\", \"month\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "      'volume', \n",
    "\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"group_id\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "  )\n",
    "    \n",
    "    validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "    # early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "    lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "    logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=25,\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=50, \n",
    "        callbacks=[lr_logger],\n",
    "        logger=logger,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=0.03,\n",
    "        hidden_size=16,\n",
    "        attention_head_size=2,\n",
    "        dropout=0.1,\n",
    "        hidden_continuous_size=8,\n",
    "        loss=QuantileLoss(),\n",
    "        log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "tft = train_model(trainer,tft,train_dataloader,val_dataloader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(tft, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (just to confirm it works)\n",
    "tft.load_state_dict(torch.load(MODEL_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Lists to store predictions and actual values\n",
    "all_preds = []\n",
    "all_ys = []\n",
    "\n",
    "# Set starting point to only use the last 1000 data points\n",
    "start_point = len(data) - 1000 - 24  # -24 to account for the 24-step lookahead\n",
    "\n",
    "# Iterate over the specified range\n",
    "for i in range(start_point, len(data) - 24):\n",
    "    # Create the dataset for the next 24 points of data\n",
    "    test = TimeSeriesDataSet.from_dataset(training, data[i:i+24], predict=True, stop_randomization=True)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = tft.predict(test, return_y=True, trainer_kwargs=dict(accelerator=\"cpu\"))\n",
    "\n",
    "    # Store predictions and actual values\n",
    "    all_preds.append(predictions.output[0][0].item())\n",
    "    all_ys.append(predictions.y[0][0][0].item())\n",
    "\n",
    "# Add predictions to the DataFrame for the last 1000 data points\n",
    "data.loc[start_point+24:, 'predictions'] = all_preds\n",
    "data.loc[start_point+24:, 'predictions_y'] = all_ys\n",
    "data['p_class']=-1\n",
    "test_data=data.tail(1000)\n",
    "test_data['predictions_y']=test_data['predictions_y'].shift(-6)\n",
    "test_data['predictions']=test_data['predictions'].shift(-6)\n",
    "test_data=test_data.iloc[:-6]\n",
    "backtesting_signals = generate_signals(data=test_data['close'].values,n=0.12,pred=test_data['predictions'].values)\n",
    "backtesting_signals=np.array(backtesting_signals)\n",
    "test_data['signals']=backtesting_signals\n",
    "test_data.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results= backtest('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsslm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
